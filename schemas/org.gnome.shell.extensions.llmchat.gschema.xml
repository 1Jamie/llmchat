<?xml version="1.0" encoding="UTF-8"?>
<schemalist>
  <schema id="org.gnome.shell.extensions.llmchat" path="/org/gnome/shell/extensions/llmchat/">
    <key name="service-provider" type="s">
      <default>'openai'</default>
      <summary>AI Service Provider</summary>
      <description>The service provider to use for AI interactions (openai, gemini, anthropic,
        llama, ollama).</description>
      <choices>
        <choice value='openai' />
        <choice value='gemini' />
        <choice value='anthropic' />
        <choice value='llama' />
        <choice value='ollama' />
      </choices>
    </key>
    <key name="openai-api-key" type="s">
      <default>''</default>
      <summary>OpenAI API Key</summary>
      <description>API key for OpenAI and compatible services.</description>
    </key>
    <key name="openai-model" type="s">
      <default>'gpt-3.5-turbo'</default>
      <summary>OpenAI Model</summary>
      <description>The model to use for OpenAI (e.g., gpt-3.5-turbo, gpt-4).</description>
      <choices>
        <choice value='gpt-3.5-turbo' />
        <choice value='gpt-4' />
      </choices>
    </key>
    <key name="openai-temperature" type="d">
      <default>0.7</default>
      <range min="0.0" max="2.0" />
      <summary>OpenAI Temperature</summary>
      <description>Controls randomness: lower values make the output more deterministic (0.0-2.0).</description>
    </key>
    <key name="gemini-api-key" type="s">
      <default>''</default>
      <summary>Google Gemini API Key</summary>
      <description>API key for Google Gemini service.</description>
    </key>
    <key name="gemini-temperature" type="d">
      <default>0.7</default>
      <range min="0.0" max="2.0" />
      <summary>Gemini Temperature</summary>
      <description>Controls randomness: lower values make the output more deterministic (0.0-2.0).</description>
    </key>
    <key name="gemini-model" type="s">
      <default>'gemini-2.0-flash-001'</default>
      <summary>Gemini Model</summary>
      <description>The model to use for Gemini (e.g., gemini-2.0-flash-001, gemini-pro).</description>
      <choices>
        <choice value='gemini-2.0-flash-001' />
        <choice value='gemini-pro' />
      </choices>
    </key>
    <key name="anthropic-api-key" type="s">
      <default>''</default>
      <summary>Anthropic API Key</summary>
      <description>API key for Anthropic service.</description>
    </key>
    <key name="anthropic-model" type="s">
      <default>'claude-2'</default>
      <summary>Anthropic Model</summary>
      <description>The model to use for Anthropic (e.g., claude-2, claude-instant-1).</description>
      <choices>
        <choice value='claude-2' />
        <choice value='claude-instant-1' />
      </choices>
    </key>
    <key name="anthropic-temperature" type="d">
      <default>0.7</default>
      <range min="0.0" max="1.0" />
      <summary>Anthropic Temperature</summary>
      <description>Controls randomness: lower values make the output more deterministic (0.0-1.0).</description>
    </key>
    <key name="anthropic-max-tokens" type="i">
      <default>500</default>
      <summary>Anthropic Max tokens</summary>
      <description>Max tokens to sample from the Anthropic Model</description>
    </key>
    <key name="llama-server-url" type="s">
      <default>'http://localhost:8080'</default>
      <summary>Llama Server URL</summary>
      <description>Server URL for local Llama instance (OpenAI-compatible API).</description>
    </key>
    <key name="llama-model-name" type="s">
      <default>'llama'</default>
      <summary>Llama Model Name</summary>
      <description>The model name to use when calling the Llama server API (e.g., llama-2-7b-chat).</description>
    </key>
    <key name="llama-temperature" type="d">
      <default>0.7</default>
      <range min="0.0" max="2.0" />
      <summary>Llama Temperature</summary>
      <description>Controls randomness: lower values make the output more deterministic (0.0-2.0).</description>
    </key>
    <key name="max-response-length" type="i">
      <default>2000</default>
      <summary>Maximum Response Length</summary>
      <description>The maximum length of responses in characters.</description>
    </key>
    <key name="ollama-server-url" type="s">
      <default>'http://localhost:11434'</default>
      <summary>Ollama Server URL</summary>
      <description>Server URL for local Ollama instance.</description>
    </key>
    <key name="ollama-model-name" type="s">
      <default>'llama2'</default>
      <summary>Ollama Model Name</summary>
      <description>The model name to use when calling the Ollama server API (e.g., llama2, mistral,
        codellama).</description>
    </key>
    <key name="ollama-temperature" type="d">
      <default>0.7</default>
      <summary>Ollama Temperature</summary>
      <description>Controls randomness: lower values make the output more deterministic (0.0-2.0).</description>
    </key>
    <key name="hide-thinking" type="b">
      <default>false</default>
      <summary>Hide Thinking Messages</summary>
      <description>Hide the "thinking" messages while waiting for responses</description>
    </key>
    <key name="max-context-tokens" type="i">
      <default>2000</default>
      <range min="500" max="100000" />
      <summary>Maximum tokens for chat context</summary>
      <description>
        The maximum number of tokens to include in the chat context sent to the LLM. Auto-limited by provider capabilities: OpenAI (8k), Anthropic (100k), Gemini (30k), Local models (4k). Higher values provide more context but may be slower.
      </description>
    </key>
    <key name="brave-search-api-key" type="s">
      <default>''</default>
      <summary>Brave Search API Key</summary>
      <description>API key for Brave Search service. Get your key from https://brave.com/search/api/</description>
    </key>
    <key name="window-states" type="s">
      <default>'[]'</default>
      <summary>Window States</summary>
      <description>JSON string containing saved window states and positions</description>
    </key>
    <key name="trigger-reindex" type="b">
      <default>false</default>
      <summary>Trigger reindexing of chat history</summary>
      <description>When enabled, triggers a full reindex of all chat history and search data. Will be automatically disabled after reindexing completes.</description>
    </key>
    <key name="monitor-states" type="s">
      <default>'[]'</default>
      <summary>Monitor States</summary>
      <description>JSON string containing saved monitor states and layouts</description>
    </key>
    <key name="log-level" type="s">
      <default>'info'</default>
      <summary>Logging Level</summary>
      <description>Controls the verbosity of logging output: error (minimal), warn (warnings and errors), info (standard), debug (verbose)</description>
      <choices>
        <choice value='error' />
        <choice value='warn' />
        <choice value='info' />
        <choice value='debug' />
      </choices>
    </key>
    <key name="memory-verbosity" type="s">
      <default>'balanced'</default>
      <summary>Memory System Verbosity</summary>
      <description>Controls the verbosity and aggressiveness of the memory system: quiet (minimal logging, fewer memories), balanced (moderate), verbose (detailed logging, more memories)</description>
      <choices>
        <choice value='quiet' />
        <choice value='balanced' />
        <choice value='verbose' />
      </choices>
    </key>
  </schema>
</schemalist>